{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q03c8u4MEGdb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "id": "dq6dZuXLMcXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle dataset path\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "path = kagglehub.dataset_download(\"marcopale/housing\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# List files\n",
        "for file in os.listdir(path):\n",
        "    print(f\"- {file}\")\n",
        "\n",
        "# Load train, test, target\n",
        "train_file = os.path.join(path, \"train.csv\")\n",
        "target_file = os.path.join(path, \"target.csv\")\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "target_df = pd.read_csv(target_file)\n",
        "\n",
        "print(f\"\\nTrain columns: {train_df.columns.tolist()[:5]}...\")\n",
        "print(f\"Target columns: {target_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "6c5MgmgwEOEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'SalePrice' in train_df.columns:\n",
        "    print(\"\\n'SalePrice' already present in train_df â€” no merge needed.\")\n",
        "    data = train_df.copy()\n",
        "else:\n",
        "    # Check correct merge key\n",
        "    common_cols = set(train_df.columns).intersection(set(target_df.columns))\n",
        "    print(\"Common columns between train and target files:\", common_cols)\n",
        "\n",
        "    if 'Id' in common_cols:\n",
        "        merge_key = 'Id'\n",
        "    elif 'PID' in common_cols:\n",
        "        merge_key = 'PID'\n",
        "    else:\n",
        "        merge_key = list(common_cols)[0]  # fallback just in case\n",
        "\n",
        "    data = pd.merge(train_df, target_df, on=merge_key, how='left')\n",
        "    print(f\"\\nMerged on key: {merge_key}\")\n",
        "\n",
        "# Handle duplicate SalePrice columns if they exist\n",
        "if 'SalePrice_x' in data.columns and 'SalePrice_y' in data.columns:\n",
        "    data = data.drop(columns=['SalePrice_x'])\n",
        "    data = data.rename(columns={'SalePrice_y': 'SalePrice'})\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = []\n",
        "if 'Order' in data.columns:\n",
        "    cols_to_drop.append('Order')\n",
        "if 'PID' in data.columns:\n",
        "    cols_to_drop.append('PID')\n",
        "\n",
        "if cols_to_drop:\n",
        "    data = data.drop(columns=cols_to_drop)\n",
        "    print(f\"Dropped identifier columns: {cols_to_drop}\")\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Shape: {data.shape}\")\n",
        "print(f\"Columns: {data.columns.tolist()[:10]}...\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "zwNrAbhoM5Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the Dataset\n",
        "print(\"Dataset Information\")\n",
        "print(data.info())\n",
        "print(\"\\nColumn Names:\")\n",
        "print(data.columns.tolist())"
      ],
      "metadata": {
        "id": "6VpkoHDlNBdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Missing Values\n",
        "print(\"Data Preprocessing - Handling Missing Values\")\n",
        "missing_counts = data.isnull().sum()\n",
        "missing_pct = (missing_counts / len(data)) * 100\n",
        "\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing_Count': missing_counts,\n",
        "    'Percentage': missing_pct\n",
        "}).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 columns with missing values:\")\n",
        "print(missing_info[missing_info['Missing_Count'] > 0].head(10))\n",
        "\n",
        "# Handle Missing Values\n",
        "# Drop columns with more than 50% missing values\n",
        "high_missing = missing_info[missing_info['Percentage'] > 50].index.tolist()\n",
        "if 'SalePrice' in high_missing:\n",
        "    high_missing.remove('SalePrice')\n",
        "\n",
        "if high_missing:\n",
        "    print(f\"\\nDropping {len(high_missing)} columns with >50% missing data\")\n",
        "    data = data.drop(columns=high_missing)\n",
        "\n",
        "# Fill numeric columns with median\n",
        "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if col != 'SalePrice' and data[col].isnull().sum() > 0:\n",
        "        data[col].fillna(data[col].median(), inplace=True)\n",
        "\n",
        "# Fill categorical columns with mode\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if data[col].isnull().sum() > 0:\n",
        "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f\"\\nMissing values after handling: {data.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "id": "aWdKCrWQNMKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Categorical Variables\n",
        "print(\"Encoding Categorical Variables\")\n",
        "\n",
        "# Keep only categorical columns with fewer than 15 unique values\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    if col in data.columns and data[col].nunique() <= 15:\n",
        "        data[col] = le.fit_transform(data[col].astype(str))\n",
        "        print(f\"Encoded: {col}\")\n",
        "    elif col in data.columns:\n",
        "        print(f\"Dropped high cardinality column: {col}\")\n",
        "        data = data.drop(columns=[col])\n"
      ],
      "metadata": {
        "id": "bfSwZtHjNTck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Statistical Summary\n",
        "print(\" Exploratory Data Analysis (EDA)\")\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(data.describe())\n"
      ],
      "metadata": {
        "id": "97XIydasNWFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target Variable Distribution\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(data['SalePrice'], bins=50, edgecolor='black', color='skyblue')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of House Prices')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.boxplot(data['SalePrice'])\n",
        "plt.ylabel('Sale Price')\n",
        "plt.title('Boxplot of House Prices')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(np.log(data['SalePrice']), bins=50, edgecolor='black', color='green')\n",
        "plt.xlabel('Log(Sale Price)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Log-Transformed Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('price_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\nSaved: price_distribution.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53uKwaOpNYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Analysis\n",
        "print(\"Correlation with Sale Price\")\n",
        "correlations = data.corr()['SalePrice'].sort_values(ascending=False)\n",
        "print(\"\\nTop 10 features correlated with SalePrice:\")\n",
        "print(correlations.head(11))"
      ],
      "metadata": {
        "id": "Fy4VarBjNarh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "top_features = correlations.abs().sort_values(ascending=False).head(11).index\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(data[top_features].corr(), annot=True, cmap='coolwarm',\n",
        "            center=0, fmt='.2f', square=True, linewidths=1)\n",
        "plt.title('Correlation Matrix - Top 10 Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Saved: correlation_matrix.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GCrm4JnxNeZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature vs Target Scatter Plots\n",
        "top_4 = correlations.iloc[1:5].index\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(top_4):\n",
        "    axes[i].scatter(data[feature], data['SalePrice'], alpha=0.5)\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Sale Price')\n",
        "    axes[i].set_title(f'SalePrice vs {feature}\\n(Corr: {correlations[feature]:.3f})')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_relationships.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Saved: feature_relationships.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VekKxxLzNhQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle Outliers\n",
        "print(\"Handling Outliers\")\n",
        "# --- 1. Remove extreme outliers in GrLivArea (common Kaggle fix)\n",
        "initial_shape = data.shape\n",
        "data = data[data['Gr Liv Area'] < 4000]\n",
        "\n",
        "print(f\"Dropped {initial_shape[0] - data.shape[0]} extreme Gr Liv Area outliers\")\n",
        "\n",
        "# --- 2. Log-transform SalePrice (to reduce skewness)\n",
        "data['SalePrice'] = np.log1p(data['SalePrice'])\n",
        "\n",
        "print(\"Applied log transformation to SalePrice\")\n",
        "\n",
        "# --- 3. Log-transform skewed numeric features\n",
        "numeric_feats = data.select_dtypes(include=[np.number]).drop(columns=['SalePrice']).columns\n",
        "skewness = data[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
        "\n",
        "skewed_features = skewness[abs(skewness) > 0.75].index\n",
        "print(f\"Log-transforming {len(skewed_features)} skewed features\")\n",
        "\n",
        "for feat in skewed_features:\n",
        "    data[feat] = np.log1p(data[feat])\n",
        "\n",
        "print(\"Outlier handling complete!\")"
      ],
      "metadata": {
        "id": "gJ58CBqQNk3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into Features and Target\n",
        "print(\"Preparing Data for Modeling\")\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('SalePrice', axis=1)\n",
        "y = data['SalePrice']\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "mVu6wp_zNlf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Split into Training and Test Sets (80-20)\n",
        "print(\"Splitting Data: 80% Training, 20% Testing\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Training set: {X_train.shape[0]} samples (80%)\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples (20%)\")\n"
      ],
      "metadata": {
        "id": "bWRQL9ItNqe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Linear Regression Model\n",
        "print(\"Training Linear Regression Model\")\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"\\nIntercept: ${model.intercept_:,.2f}\")\n"
      ],
      "metadata": {
        "id": "TDn2eXd7NsXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Top Feature Coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)"
      ],
      "metadata": {
        "id": "fiQCkxdnNwlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "print(\"Making Predictions\")\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "print(\"Predictions completed!\")"
      ],
      "metadata": {
        "id": "YcHuu0RcNzuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Performance\n",
        "print(\"Model Evaluation\")\n",
        "\n",
        "# Training set metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# Test set metrics\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nTRAINING SET PERFORMANCE:\")\n",
        "print(f\"  Mean Squared Error (MSE): {train_mse:,.2f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): ${train_rmse:,.2f}\")\n",
        "print(f\"  R-squared (RÂ²): {train_r2:.4f}\")\n",
        "\n",
        "print(\"\\nTEST SET PERFORMANCE:\")\n",
        "print(f\"  Mean Squared Error (MSE): {test_mse:,.2f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): ${test_rmse:,.2f}\")\n",
        "print(f\"  R-squared (RÂ²): {test_r2:.4f}\")\n",
        "\n",
        "print(\"\\nINTERPRETATION:\")\n",
        "print(f\"  The model explains {test_r2*100:.2f}% of variance in house prices\")\n",
        "print(f\"  Average prediction error: ${test_rmse:,.2f}\")"
      ],
      "metadata": {
        "id": "zJSMjKyBN26k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Predicted vs Actual Prices\n",
        "\n",
        "print(\"Visualizing Results\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Predicted vs Actual\n",
        "axes[0].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Sale Price')\n",
        "axes[0].set_ylabel('Predicted Sale Price')\n",
        "axes[0].set_title(f'Predicted vs Actual House Prices\\n(RÂ² = {test_r2:.4f})')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual Plot\n",
        "residuals = y_test - y_test_pred\n",
        "axes[1].scatter(y_test_pred, residuals, alpha=0.6, edgecolors='k', linewidths=0.5)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Sale Price')\n",
        "axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
        "axes[1].set_title('Residual Plot')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prediction_results.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Saved: prediction_results.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ispsPgSVN6EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Additional Visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Residual Distribution\n",
        "axes[0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[0].axvline(x=0, color='r', linestyle='--', lw=2)\n",
        "axes[0].set_xlabel('Residual Value')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Residuals')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Feature Importance\n",
        "top_10 = coefficients.head(10)\n",
        "axes[1].barh(range(len(top_10)), top_10['Coefficient'], color='steelblue')\n",
        "axes[1].set_yticks(range(len(top_10)))\n",
        "axes[1].set_yticklabels(top_10['Feature'])\n",
        "axes[1].set_xlabel('Coefficient Value')\n",
        "axes[1].set_title('Top 10 Feature Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('additional_analysis.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Saved: additional_analysis.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vNRAIHRhN98r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Analysis and Conclusions\n",
        "print(\"Analysis & Conclusions\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "r2_diff = train_r2 - test_r2\n",
        "\n",
        "print(\"\\nMODEL PERFORMANCE ASSESSMENT:\")\n",
        "if r2_diff > 0.1:\n",
        "    print(\"\\nOVERFITTING DETECTED:\")\n",
        "    print(f\"  Training RÂ²: {train_r2:.4f}\")\n",
        "    print(f\"  Test RÂ²: {test_r2:.4f}\")\n",
        "    print(f\"  Difference: {r2_diff:.4f}\")\n",
        "    print(\"  The model performs much better on training data than test data.\")\n",
        "    print(\"  Recommendation: Use regularization (Ridge/Lasso regression)\")\n",
        "elif test_r2 < 0.5:\n",
        "    print(\"\\nUNDERFITTING DETECTED:\")\n",
        "    print(f\"  Test RÂ² is low: {test_r2:.4f}\")\n",
        "    print(\"  The model is too simple to capture the relationships in the data.\")\n",
        "    print(\"  Recommendation: Add polynomial features or use a more complex model\")\n",
        "else:\n",
        "    print(\"\\nMODEL PERFORMS WELL:\")\n",
        "    print(f\"  Training RÂ²: {train_r2:.4f}\")\n",
        "    print(f\"  Test RÂ²: {test_r2:.4f}\")\n",
        "    print(f\"  Difference: {r2_diff:.4f}\")\n",
        "    print(\"  The model generalizes reasonably well to unseen data.\")\n",
        "\n",
        "print(f\"\\nKEY FINDINGS:\")\n",
        "print(f\"  1. Model explains {test_r2*100:.1f}% of house price variance\")\n",
        "print(f\"  2. Average prediction error: ${test_rmse:,.0f}\")\n",
        "print(f\"  3. Most important features:\")\n",
        "for i, (_, row) in enumerate(coefficients.head(3).iterrows(), 1):\n",
        "    print(f\"     {i}. {row['Feature']}: ${row['Coefficient']:,.2f}\")\n",
        "\n",
        "print(\"ANALYSIS COMPLETE!\")"
      ],
      "metadata": {
        "id": "hm1iMd6CN9_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}